# Sampling
Sampling Techniques & ML Model Performance AnalysisüéØ ObjectiveThe primary goal of this project is to address class imbalance in a credit card dataset (763 legitimate vs. 9 fraudulent transactions). We explore how various sampling techniques combined with different Machine Learning models affect classification accuracy.üõ†Ô∏è Methodology & Techniques Used1. Handling Extreme Imbalance: Hybrid SamplingWith a ratio of nearly 85:1, SMOTE alone would over-interpolate the 9 minority samples. Instead, a Hybrid Approach was used:SMOTE (Oversampling): Synthetically generated new minority instances to increase the fraudulent class to a 10% ratio of the majority.Random UnderSampler: Reduced the majority class to match the newly boosted minority class, creating a perfect 1:1 balanced distribution.2. Sampling Techniques AppliedFive distinct statistical sampling methods were used to create training subsets:Simple Random Sampling: Every instance has an equal chance of selection.Systematic Sampling: Selecting every $k^{th}$ instance from the balanced dataset.Stratified Sampling: Ensures the 50/50 class ratio is maintained exactly within the sample.Cluster Sampling: Dividing the population into clusters and randomly selecting entire groups.Bootstrap Sampling: Random sampling with replacement, allowing the same instance to be selected multiple times.3. Machine Learning ModelsM1: Logistic Regression (Linear baseline).M2: Decision Tree (Non-linear, captures complex splits).M3: Random Forest (Ensemble of trees for high robustness).M4: Support Vector Classifier (SVC) (Effective in high-dimensional spaces).M5: Extra Trees Classifier (Extremely randomized trees to reduce variance).üìä Performance Comparison (Accuracy %)ModelSimpleRandomSystematicStratifiedClusterBootstrapM1: Logistic Reg94.8194.8192.2196.1097.40M2: Decision Tree100.097.4098.7097.4097.40M3: Random Forest100.097.40100.0100.0100.0M4: SVC63.6462.3463.6467.5372.73M5: Extra Trees100.097.4098.70100.0100.0üîç Key Insights & DiscussionThe "Winner": Random Forest (M3)Random Forest emerged as the most consistent model, achieving near-perfect accuracy across almost all sampling techniques. Its ensemble nature allows it to handle the synthetic patterns created by SMOTE better than single-model classifiers.The Challenge with SVC (M4)SVC performed significantly worse than other models (peaking at ~72%). This is common with SMOTE-enhanced data because SVC is highly sensitive to the spatial distribution of data points. The "synthetic" bridge created between the 9 original points and the majority class likely created a fuzzy decision boundary that SVC struggled to separate.Impact of Sampling SelectionBootstrap Sampling yielded the highest accuracy for the Logistic Regression model (97.40%), suggesting that repeating certain key instances helped the linear model define the decision boundary more clearly.Systematic Sampling showed slightly lower performance across the board, likely because the fixed interval skipped over subtle patterns in the resampled data.üöÄ ConclusionBalancing the dataset is only the first step. As demonstrated, the sampling strategy used to train the model can sway accuracy by over 10%. For highly imbalanced credit card data, Ensemble Tree-based models (M3, M5) combined with Stratified or Bootstrap sampling provide the most reliable results.
